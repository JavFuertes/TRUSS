{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img \n",
    "    style=\"position: absolute; \n",
    "           left: 60%; \n",
    "           top: 0; /* Added to ensure proper positioning */\n",
    "           height: 900px; \n",
    "           width: 40%; /* Maintain the original width */\n",
    "           object-fit: cover; /* Adjust if necessary */\n",
    "           clip-path: inset(0px 50px 0px 50px round 10px);\" \n",
    "    src=\"https://www.mathworks.com/help/examples/stats/win64/ParellelBayeianOptimizationExample_01.png\" \n",
    "/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<h1 style=\"width: 60%; color: #EC6842; font-size: 55pt;\">\n",
    "    <Strong>\n",
    "        Bayesian optimization in Truss structures optimisation\n",
    "    </Strong>\n",
    "</h1>\n",
    "\n",
    "<h2 id=\"Background\"><B>\n",
    "    Rationale for the project<a class=\"anchor-link\" href=\"#Background\">&#182;</a>\n",
    "    </B>\n",
    "</h2>\n",
    "<p style=\"text-align: justify; width: 60%; font-weight: normal;\">\n",
    "     Structures that make optimal use of the material they are made of reduces the cost and environmental impact of their construction as the amount of material required. Optimization of structural design is a challenging task because of the high number of design parameters and the relatively expensive evaluation of the suitability of any given design. Standard optimization techniques in high-dimensional design space require a very large number of possible designs that need to be evaluated. In structural analysis, where evaluating the objective function and checking the constraints involves the solution of a structural mechanics problem, e.g. with finite elements, this quickly becomes very expensive, even if the model is relatively simple from structural point of view. Bayesian optimization is a machine-learning-based optimization technique that aims to reduce the number of evaluations of the objective function through data-driven exploration of the design space with a probabilistic surrogate.\n",
    "</p>\n",
    "\n",
    "<h2 id=\"Background\">\n",
    "    <B>Objective & Description:</B><a class=\"anchor-link\" href=\"#Background\">&#182;</a>\n",
    "</h2>\n",
    "\n",
    "<div style=\"width: 60%; border-top: 4px solid #00B8C8; border-left: 4px solid #00B8C8; background-color: #FFFFFF; padding: 1em 1em 1em 1em; color: #24292E; margin: 10px 0 20px 0; box-sizing: border-box;\">\n",
    "    <div style=\"background-color: #00B8C8; color: white; padding: 0.2em 1em; margin: -1em -1em 0em -1em; font-size: 1.2em;\"><strong>Project Objective:</strong> To find the optimal truss design</div>\n",
    "    <p>\n",
    "    To achieve so this project requires to find the optimal set of nodal coordinates and cross-sectional properties. Achieving so will allow to minimize as much as possible the total weight of the structure, while satisfying a number of constraints relating to the structures natural frequencies. Achieving a low mass solutions that also satisfies the natural frequencies constraints established demonstrates a methodology to make structures more efficient and safe since we are achieving to use less material in a way that still ensures the structural integrity of the structure. Moreover, this project will also aim to explore the efficacy of current optimisation methods and potentially improvements to be achieved from implementing machine learning methods.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> X | Imports</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- BOPT ----------------------------------- #\n",
    "from botorch.utils.transforms import normalize\n",
    "from botorch.utils.transforms import unnormalize\n",
    "from botorch.models import SingleTaskGP\n",
    "#from botorch.fit import fit_gpytorch_model\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import ExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "\n",
    "from tabulate import tabulate\n",
    "from contextlib import redirect_stdout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- LOCAL IMPORTS ------------------------------ #\n",
    "sys.path.append('../TRUSS1/truss_bridge')\n",
    "from dash import Dash, dcc, html, Input, Output\n",
    "from TRUSS_convergence import plot_truss_layout, get_truss_data_for_iteration, loss_function_figure\n",
    "\n",
    "sys.path.append('../TRUSS1/pyJive/')\n",
    "import main\n",
    "from utils import proputils as pu\n",
    "from names import GlobNames as gn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 0 | Classes & Dependencies</strong>\n",
    "\n",
    "This project initially had two main classes:\n",
    "\n",
    "1. **TRUSS**:\n",
    "<br>A class primarily concerned with stroing the TRUSSES characteristics and processing function allowing for the manipulation either for optimisation or data exploration.\n",
    "Most importantly this class also included the loss function from which the algorithm relies upon. The structure of this function is the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRUSS():\n",
    "    def __init__(self, file_path, file_name, output_path, write_name, violation_penalty, scalers, bounds, lower_area=4e-3, freq_constraints=[20, 40, 60], mass_weight=1, freq_weight=1):\n",
    "        # Configuration Parameters\n",
    "        self.config = {\n",
    "            \"read_file_path\": file_path,\n",
    "            \"read_file_name\": file_name,\n",
    "            \"write_file_path\": output_path,\n",
    "            \"write_file_name\": write_name,\n",
    "            \"violation_penalty\": violation_penalty,\n",
    "            \"scalers\": {\"y\": scalers[0], \"a\": scalers[1]},\n",
    "            \"lower_area\": lower_area,\n",
    "            \"bounds\" : bounds,\n",
    "            \"freq_constraints\": freq_constraints,\n",
    "            \"mass_weight\": mass_weight,\n",
    "            \"freq_weight\": freq_weight,\n",
    "            \"density\": 7850,  # kg/m^3 As per Garnachos\n",
    "        }\n",
    "\n",
    "        self.state = {\n",
    "            \"nodes\": {},\n",
    "            \"connectivity\": None,\n",
    "            \"member_df\": None,\n",
    "            \"y_coordinates\": None,\n",
    "            \"x_coordinates\": None,\n",
    "            \"cross_sections\": None,\n",
    "            \"lengths\": None,\n",
    "            \"mass_norm\": 360,\n",
    "            \"monitor_df\": None\n",
    "        }\n",
    "\n",
    "        self.columns = [\"Metric\", \"Total Loss\", \"Natural Frequencies\", \"Mass of Truss\", \"Normalized Mass Loss\", \"Frequency Penalty\", \"Constraint Violation (Loss No W)\"]\n",
    "        self.state[\"monitor_df\"] = pd.DataFrame(columns=self.columns)\n",
    "    # -------------------------- Import & Writing Files -------------------------- #\n",
    "    def read_geom_file(self):\n",
    "        path = os.path.normpath(os.path.join(self.config[\"read_file_path\"], self.config[\"read_file_name\"]))\n",
    "        with open(path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        nodes = {}\n",
    "        members = []\n",
    "        section = None\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if line.startswith('node:'):\n",
    "                section = 'node'\n",
    "            elif line.startswith('member:'):\n",
    "                section = 'member'\n",
    "            else:\n",
    "                if section == 'node' and line:\n",
    "                    parts = line.split()\n",
    "                    node_number = int(parts[0])\n",
    "                    node_data = tuple(map(float, parts[1:]))\n",
    "                    nodes[node_number] = node_data\n",
    "                elif section == 'member' and line:\n",
    "                    member_data = tuple(map(int, line.split()))\n",
    "                    members.append(member_data)\n",
    "\n",
    "        self.state[\"nodes\"] = nodes\n",
    "        self.state[\"member_df\"] = pd.DataFrame(members, columns=['Node #1', 'Node #2', 'Number of Elements', 'Cross-section Type'])\n",
    "\n",
    "        connectivity = self.state[\"member_df\"].to_numpy()\n",
    "        self.state[\"connectivity\"] = connectivity[:, :2]\n",
    "        self.state[\"y_coordinates\"] = np.array([value[1] for _, value in nodes.items()])\n",
    "        self.state[\"x_coordinates\"] = np.array([value[0] for _, value in nodes.items()])\n",
    "\n",
    "    def write_geom_file(self,path_,write_file): \n",
    "        path = os.path.normpath(os.path.join(path_,write_file))\n",
    "        file = open(path)\n",
    "\n",
    "        for i, (x, _) in self.state[\"nodes\"].items():\n",
    "            self.state[\"nodes\"][i] = (x, self.state[\"y_coordinates\"][i])\n",
    "\n",
    "        geom = 'node: node #, x-coordinate; y-coordinate\\n'\n",
    "        for node_num, coords in self.state[\"nodes\"].items():\n",
    "            geom += f\"{node_num} {' '.join(map(str, coords))}\\n\"\n",
    "        \n",
    "        connectivity = self.state[\"member_df\"].to_numpy()\n",
    "        geom += '\\nmember: Node #1; Node #2; number of elements; cross-section type\\n'\n",
    "        for row in connectivity:\n",
    "            geom += ' '.join(map(str, row)) + '\\n'\n",
    "\n",
    "        output_file_path = os.path.normpath(os.path.join(path_,'TRUSS.geom'))\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            file.write(geom)\n",
    "            \n",
    "        return file.name\n",
    "    \n",
    "    def fit_scalers(self, initial_data):\n",
    "        y_data = initial_data[:5].reshape(1, -1).T\n",
    "        a_data = initial_data[5:].reshape(1, -1).T\n",
    "        self.config[\"scalers\"][\"y\"].fit(y_data)\n",
    "        self.config[\"scalers\"][\"a\"].fit(a_data)\n",
    "\n",
    "    def scaler(self, mode, x_values, scaler_type):\n",
    "        scaler = self.config[\"scalers\"][scaler_type]\n",
    "        x_values = x_values.reshape(1, -1).T\n",
    "        if mode == 'scale':\n",
    "            scaled_values = scaler.transform(x_values)\n",
    "        elif mode == 'descale':\n",
    "            scaled_values = scaler.inverse_transform(x_values)\n",
    "        return scaled_values.flatten()\n",
    "    \n",
    "    def areas_coordinates_write_split(self, x_list):        \n",
    "        y_coordinates = np.copy(self.state[\"y_coordinates\"])  # Use a copy to avoid modifying the original array directly\n",
    "        areas = x_list[5:]\n",
    "        y_coords = x_list[:5]\n",
    "        for i, y in enumerate(y_coords, start=1):\n",
    "            y_coordinates[2 * i] = y\n",
    "            y_coordinates[20 - 2 * i] = y\n",
    "\n",
    "        self.state[\"y_coordinates\"] = y_coordinates  # Update state\n",
    "\n",
    "        return areas, y_coords\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    #                                TARGET FUNCTION                               #\n",
    "    # ---------------------------------------------------------------------------- #\n",
    "    def loss_function(self, x_list):\n",
    "        \"\"\"\n",
    "        Calculate the loss for a given set of design parameters.\n",
    "        Parameters:\n",
    "        x_list (list): A list of design parameters.\n",
    "        Returns:\n",
    "        float: The calculated loss based on mass, frequency constraints, and imaginary penalties.\n",
    "        \"\"\"\n",
    "        if isinstance(x_list,torch.Tensor):\n",
    "            x_list = x_list.detach().cpu().numpy().flatten()\n",
    "            self.temporary = x_list\n",
    "        \n",
    "        A_list, y_list = self.areas_coordinates_write_split(x_list)\n",
    "        mass_truss = self.mass_truss(A_list, y_list)\n",
    "        freq1, freq2, freq3 = self.eigenfrequencies(A_list)\n",
    "        frequencies = np.array([freq1, freq2, freq3])\n",
    "\n",
    "        freq_penalty = 0\n",
    "        all_positive = True\n",
    "        for f, t in zip(frequencies, self.config[\"freq_constraints\"]):\n",
    "            diff = f - t\n",
    "            penalty_factor = 1 if diff >= 0 else self.config[\"violation_penalty\"]\n",
    "            freq_penalty += penalty_factor * (np.abs(diff) / t)\n",
    "            if diff < 0:\n",
    "                all_positive = False\n",
    "        \n",
    "        if all_positive:\n",
    "            self.state[\"mass_norm\"] = min(mass_truss, self.state[\"mass_norm\"])\n",
    "\n",
    "        normalized_mass_penalty = (mass_truss - self.state[\"mass_norm\"]) / self.state[\"mass_norm\"]\n",
    "        normalized_freq_penalty = freq_penalty / len(self.config[\"freq_constraints\"])\n",
    "        constraint_violation = normalized_freq_penalty + normalized_mass_penalty\n",
    "\n",
    "        LOSS = (self.config[\"mass_weight\"] * normalized_mass_penalty +\n",
    "                self.config[\"freq_weight\"] * normalized_freq_penalty)\n",
    "        # -------------------------- Optimisation monitoring ------------------------- #\n",
    "        optmonitor_data = [\n",
    "            [\"Metric\",\"Total Loss\", \"Natural Frequencies\", \"Mass of Truss\", \"Normalized Mass Loss\", \"Frequency Penalty\", \"Constraint Violation (Loss No W)\"],\n",
    "            [\"Value\",LOSS, frequencies, mass_truss, normalized_mass_penalty, normalized_freq_penalty, constraint_violation]\n",
    "        ]\n",
    "        print(f'{tabulate(optmonitor_data, headers=\"firstrow\", tablefmt=\"grid\")}\\n')\n",
    "        \n",
    "        row_data = optmonitor_data[1][0:]\n",
    "        new_data = pd.DataFrame([row_data], columns=self.columns)\n",
    "        self.state[\"monitor_df\"] = pd.concat([self.state[\"monitor_df\"], new_data], ignore_index=True)\n",
    "         \n",
    "        return LOSS\n",
    "\n",
    "    # ----------------------------- SUPPORT FUNCTIONS ---------------------------- #\n",
    "    def eigenfrequencies(self,A_list):\n",
    "        path = os.path.normpath(os.path.join(self.config[\"read_file_path\"], 'bridge_frequency.pro'))\n",
    "        \n",
    "        props = pu.parse_file(path)\n",
    "        props['model']['truss']['area'] = A_list\n",
    "        props['init']['mesh']['file'] = self.write_geom_file(self.config[\"write_file_path\"], self.config[\"write_file_name\"])\n",
    "        \n",
    "        trap = io.StringIO()\n",
    "        with redirect_stdout(trap):\n",
    "            globdat = main.jive(props)\n",
    "        \n",
    "        return (globdat[gn.EIGENFREQS][0:3] / 2 / np.pi)\n",
    "\n",
    "    def mass_truss(self,areas, y_list):\n",
    "        x_coordinates = self.state[\"x_coordinates\"]\n",
    "        y_coordinates = self.state[\"y_coordinates\"]\n",
    "        \n",
    "        for i, y in enumerate(y_list, start=1):\n",
    "            y_coordinates[2 * i] = y\n",
    "            y_coordinates[20 - 2 * i] = y\n",
    "\n",
    "        coordinates = list(zip(x_coordinates, y_coordinates))\n",
    "        lengths = np.zeros(len(self.state[\"connectivity\"]))\n",
    "        for i, (node1, node2) in enumerate(self.state[\"connectivity\"]):\n",
    "            x1, y1 = coordinates[node1]  # Adjusting index to start from 0\n",
    "            x2, y2 = coordinates[node2]\n",
    "            lengths[i] = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "        n_elements = self.state[\"member_df\"].iloc[:,2].sum()\n",
    "        areas_i = np.zeros(n_elements)\n",
    "        cross_section_number = self.state[\"member_df\"].iloc[:,-1] \n",
    "        \n",
    "        for i,item in enumerate(cross_section_number):\n",
    "                areas_i[i] = areas[item]\n",
    "        self.temporary = areas_i\n",
    "        self.state[\"cross_sections\"] = areas[:15]\n",
    "        self.state[\"lengths\"] = lengths\n",
    "\n",
    "        volumes = lengths * areas_i\n",
    "        total_volume = np.sum(volumes)\n",
    "        dens = 7800\n",
    "        mass = total_volume * dens\n",
    "\n",
    "        return mass\n",
    "    # ----------------------------------- OTHER ---------------------------------- #\n",
    "    def Write_call_state(self, x, final= True):\n",
    "        if final:\n",
    "            mask = self.state[\"monitor_df\"][\"Natural Frequencies\"][:].apply(lambda x: (x >= self.config[\"freq_constraints\"]).all())\n",
    "            ind = self.state[\"monitor_df\"][mask][\"Mass of Truss\"].idxmin()\n",
    "            min_mass_iter = self.state[\"monitor_df\"].iloc[ind]\n",
    "            x_update = unnormalize(x[ind], bounds= self.config[\"bounds\"].T)\n",
    "        else:\n",
    "            x_update = unnormalize(x, bounds= self.config[\"bounds\"].T)\n",
    "        # ------------------------------- Update state ------------------------------- #\n",
    "        areas ,y_coords = self.areas_coordinates_write_split(x_update) # Serves to update coordinates\n",
    "        _ = self.mass_truss(areas, y_coords) # Serves to update cross_sections and lengths\n",
    "        path_wr = self.write_geom_file(self.config[\"write_file_path\"], 'TRUSS.geom') # Serves to update GEOM\n",
    "        # --------------------------------- Call geom -------------------------------- #\n",
    "        path = os.path.normpath(os.path.join(self.config[\"write_file_path\"],'bridge_frequency_truss.pro'))\n",
    "        props = pu.parse_file(path)\n",
    "        props['model']['truss']['area'] = self.state[\"cross_sections\"]\n",
    "        props['init']['mesh']['file'] = path_wr\n",
    "        trap = io.StringIO()\n",
    "        with redirect_stdout(trap):\n",
    "            globdat = main.jive(props)\n",
    "        if final:\n",
    "            print(f'TRUSS NFs:{(globdat[gn.EIGENFREQS][0:3] / 2 / np.pi)} and Mass:{min_mass_iter}')\n",
    "\n",
    "    def Initial_guess(self, optim_dims):\n",
    "        sampled_points = np.zeros(len(optim_dims))\n",
    "        for i, dim_index in enumerate(optim_dims):\n",
    "            lower_bound, upper_bound = self.config[\"bounds\"][i]\n",
    "            sampled_points[i] = np.random.uniform(lower_bound, upper_bound)\n",
    "        return sampled_points\n",
    "    \n",
    "    def objective_function(self, x):\n",
    "        \"\"\"\n",
    "        Evaluates the objective function using another object's method.\n",
    "        \"\"\"\n",
    "        return self.loss_function(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Bayesian optimiser**: \n",
    "<br>A class which was the self-developed optimiser originally intended to use. Due to project time constraints this was left aside since the convergence time of this solution was not so good and more resembled and informed random walk. Furhter exploration of the behaviour of the optimiser seemed to show difficulties in the gaussian process in capturing the uncertainty in the multi-dimensional solution space which did not allow for efficient exploration.\n",
    "\n",
    "    The code for the TRUSS´s project bayesian optimiser class can be found in [TRUSS BOPT](truss_bridge/TRUSS_Bopt.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 1 | Inititation of objects and attributes</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Definitions ------------------------------- #\n",
    "read_file_path = '../TRUSS1/truss_bridge/data'\n",
    "read_file = 'bridge.geom'\n",
    "write_file_path =  '../TRUSS1/truss_bridge/data/@TRUSS'\n",
    "write_file = 'TRUSS.geom'\n",
    "optim_dims = [i for i in range(20)]  \n",
    "lower_area = 40e-4\n",
    "violation_factor = 6\n",
    "freq_constraints = [20,40,60]\n",
    "mass_weight = 2.5\n",
    "freq_weight = 6.0\n",
    "A_bound = (0.0001, 0.01)\n",
    "y_bound = (0.5, 2)\n",
    "scalers = [MinMaxScaler(),MinMaxScaler()] \n",
    "bounds = np.array([y_bound if index <= 4 else A_bound for index in optim_dims])\n",
    "bounds[5] = (lower_area - A_bound[0], lower_area + A_bound[0])\n",
    "\n",
    "TRUSS1= TRUSS(read_file_path,\n",
    "               read_file,\n",
    "                write_file_path,\n",
    "                write_file,\n",
    "                violation_factor,\n",
    "                scalers,\n",
    "                bounds,\n",
    "                freq_constraints= freq_constraints, \n",
    "                mass_weight= mass_weight, \n",
    "                freq_weight= freq_weight)\n",
    "TRUSS1.read_geom_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- Initiation -------------------------------- #\n",
    "n_samples =  10\n",
    "x_list = []\n",
    "y_list = []\n",
    "x_list_sc = []\n",
    "x_0 = np.empty(len(optim_dims))\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x_c = np.copy(x_0) # Reset seed\n",
    "    x_random = TRUSS1.Initial_guess(optim_dims)\n",
    "    x_c[optim_dims] = x_random\n",
    "    x_list.append(x_c)\n",
    "# \n",
    "trap = io.StringIO()\n",
    "with redirect_stdout(trap):\n",
    "    for x in x_list:\n",
    "        LOSS = TRUSS1.loss_function(x)\n",
    "        y_list.append(LOSS)\n",
    "# \n",
    "# ---------------------------- INITIATE OPTIMISER ---------------------------- #\n",
    "bounds = torch.tensor(bounds.T)  \n",
    "y_init_t = torch.tensor(y_list).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 2 | Optimisation </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 2.1 | Single task Optimisation </strong>\n",
    "\n",
    "After some experimentation the best performing algorithm despite not having or relying on as strong probabilistic interpretation as the SAAS model. This has been true for this dimension space since the computational complexity of the probabilistic model dowgrades its convergence time, in other words the searched solution space can be solved through a simpler bayesian optimisation approach. Other findings included how the algorithm coverges faster to a solution with smaller batch sizes this is potentially since the bounds and constraints introduced are already quite small and also potentially indicating there isnt that many locations with local minima and therefore a smoother overall solution space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleBOPT(Y_init_single, X_init_single, n_iter, batch_size, Nrestats, objective_function):\n",
    "    gp_model = SingleTaskGP(X_init_single, Y_init_single)\n",
    "    mll = ExactMarginalLogLikelihood(gp_model.likelihood, gp_model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    gp_model.set_train_data(inputs=X_init_single, targets=Y_init_single.flatten(), strict=False)\n",
    "    \n",
    "    for iteration in range(n_iter):\n",
    "        print(f'Iteration: {iteration} Best loss = {Y_init_single.min().item():.2f}\\n')\n",
    "        acq_func = ExpectedImprovement(model=gp_model, best_f=Y_init_single.min(), maximize=False)\n",
    "        new_x, _ = optimize_acqf(\n",
    "            acq_function=acq_func,\n",
    "            bounds=bounds,\n",
    "            q=1,\n",
    "            num_restarts= Nrestats,\n",
    "            raw_samples= batch_size,\n",
    "        )\n",
    "        new_x_unnorm = unnormalize(new_x, bounds=bounds)\n",
    "        new_y = objective_function(new_x_unnorm)\n",
    "        new_y = torch.tensor(new_y, dtype=torch.float32).reshape(1,-1)\n",
    "        X_init_single = torch.cat([X_init_single, new_x])\n",
    "        Y_init_single = torch.cat([Y_init_single, new_y])\n",
    "        \n",
    "        gp_model.set_train_data(inputs=X_init_single, targets=Y_init_single.flatten(), strict=False)\n",
    "        fit_gpytorch_model(mll)\n",
    "    return X_init_single, Y_init_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the following cell we run the BOPT,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function = TRUSS1.loss_function\n",
    "n_iter = 50\n",
    "batch_size = 15\n",
    "Nrestats = 10\n",
    "\n",
    "X_init_single , Y_init_single = SingleBOPT(Y_init_single, X_init_single, n_iter, batch_size, Nrestats, objective_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 2.2 | Read and Write Data </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# -------------------------- SAVE DATA !!DO NOT RUN -------------------------- #\n",
    "# file_name = \"TRUSS1_bopt_data.pkl\"\n",
    "# path_s = os.path.normpath(os.path.join(write_file_path, file_name))\n",
    "\n",
    "# data_single = {\n",
    "#     \"Y_init_single\": Y_init_single,\n",
    "#     \"X_init_single\": X_init_single,\n",
    "#     \"n_iter\": n_iter,\n",
    "#     \"batch_size\": batch_size,\n",
    "#     \"Nrestats\": Nrestats,\n",
    "#     \"X_single\": X_init_single,\n",
    "#     \"Y_single\": Y_init_single,\n",
    "#     \"TRUSS\": TRUSS1\n",
    "# }\n",
    "# data_to_save = [data_single]\n",
    "\n",
    "# with open(path_s, 'wb') as file:\n",
    "#     pickle.dump(data_to_save, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "read_file_path = write_file_path =  '../TRUSS1/truss_bridge/data/@TRUSS/visualisation'\n",
    "read_file = write_file = 'TRUSS.geom'\n",
    "\n",
    "TRUSSDummy = TRUSS(read_file_path,\n",
    "               read_file,\n",
    "                write_file_path,\n",
    "                write_file,\n",
    "                violation_factor,\n",
    "                scalers,\n",
    "                bounds,\n",
    "                freq_constraints= freq_constraints, \n",
    "                mass_weight= mass_weight, \n",
    "                freq_weight= freq_weight)\n",
    "TRUSSDummy.read_geom_file()\n",
    "\n",
    "file_name = \"TRUSS1_bopt_data.pkl\"\n",
    "write_file_path =  '../TRUSS1/truss_bridge/data/@TRUSS/.pkl'\n",
    "path_s = os.path.normpath(os.path.join(write_file_path, file_name))\n",
    "with open(path_s, 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "# --------------------------------- variables -------------------------------- #\n",
    "data_single = loaded_data[0]\n",
    "Y_init_single = data_single[\"Y_init_single\"]\n",
    "X_init_single = data_single[\"X_init_single\"]\n",
    "n_iter = data_single[\"n_iter\"]\n",
    "batch_size = data_single[\"batch_size\"]\n",
    "Nrestats = data_single[\"Nrestats\"]\n",
    "X_init_single = data_single[\"X_single\"]\n",
    "Y_init_single = data_single[\"Y_single\"]\n",
    "TRUSSDummy = data_single[\"TRUSS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <strong> 3 | Results </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 3.1 | TRUSS Run </strong>\n",
    "\n",
    "The following figure illustrates the explored truss shapes  by the bayesian optimiser in its convergence path. As a note, it can be seen that the lowest mass and the deemed best solution in the following code cells is not neccessarily the solution with the lowest loss function. This illustrates how there can still be improvements made to the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "total_loss = TRUSSDummy.state[\"monitor_df\"][\"Total Loss\"]\n",
    "TRUSSDummy.config[\"write_file_path\"] = '../TRUSS1/truss_bridge/data/@TRUSS/visualisation'\n",
    "fig = loss_function_figure(total_loss)\n",
    "\n",
    "layout={\n",
    "    'title': 'Hover over a point in the loss function to see the truss layout',\n",
    "    'height': 500,  \n",
    "    'width': 1000,  \n",
    "    'plot_bgcolor': 'white',  \n",
    "    'paper_bgcolor': 'white',  \n",
    "}\n",
    "\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    html.Div([\n",
    "        dcc.Graph(id='truss-layout', figure=go.Figure()),  \n",
    "    ], style={'display': 'inline-block', 'width': '55%'}),\n",
    "    \n",
    "    html.Div([\n",
    "        dcc.Graph(id='loss-function', figure=fig),\n",
    "    ], style={'display': 'inline-block', 'width': '40%'}),\n",
    "], style={'width': '1500px'})\n",
    "\n",
    "@app.callback(\n",
    "    Output('truss-layout', 'figure'),\n",
    "    [Input('loss-function', 'hoverData')]\n",
    ")\n",
    "\n",
    "def update_truss_layout(hoverData):\n",
    "    if hoverData is not None:\n",
    "        point_index = hoverData['points'][0]['pointIndex']\n",
    "        nodes, member_df = get_truss_data_for_iteration(TRUSSDummy, X_init_single, point_index)\n",
    "        return plot_truss_layout(nodes, member_df)\n",
    "    return go.Figure(layout= layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 3.2 | Investigating the variance in solution paths </strong>\n",
    "\n",
    "Investigating the solution path we can observe how there is a very big variance between specific runs although this is moreso uncommon it demonstrates how unlike Kanarachos perhaps our Bopt solver is not as resilient to changing solution spaces. This is particularly an issue in our case since our solution space will change depending on the hyperparameters of our loss function. It is therefore important to account well for such behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 5\n",
    "n_iter_comparison = 50\n",
    "# ------------------------- IMPORT ENSEMBLE LOOP DATA ------------------------ #\n",
    "file_name = \"TRUSS1_ensemble_data.pkl\"\n",
    "path_s = os.path.normpath(os.path.join(write_file_path, file_name))\n",
    "with open(path_s, 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "    data = loaded_data[0]\n",
    "\n",
    "results_single = data[\"results_single\"]\n",
    "mean_single =  data[\"mean_single\"]\n",
    "std_single = data[\"std_single\"]\n",
    "\n",
    "# ----------------------------- RUN ENSEMBLE LOOP ---------------------------- #\n",
    "# results_single = np.zeros((num_runs, n_iter_comparison))\n",
    "# Y_init_bpt , X_init_bpt = y_init_t, X_init_norm\n",
    "\n",
    "# for i in range(num_runs):\n",
    "#     _, Y_init_p = SingleBOPT(Y_init_bpt, X_init_bpt, n_iter_comparison, batch_size, Nrestats)\n",
    "#     results_single[i, :] = Y_init_p[n_samples:].flatten()\n",
    "# mean_single = np.mean(results_single, axis=0)\n",
    "# std_single = np.std(results_single, axis=0)\n",
    "\n",
    "results_single = results_single.T  \n",
    "mean_single = results_single.mean(axis=1)\n",
    "std_single = results_single.std(axis=1)\n",
    "confidence_interval = 1.96 * (std_single / np.sqrt(num_runs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "base_color = 'rgba(30, 136, 229,'  # RGB for blue, with variable opacity\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(num_runs):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=np.arange(n_iter_comparison),\n",
    "        y=results_single[:, i],\n",
    "        mode='lines',\n",
    "        name=f'Run {i+1}',\n",
    "        line=dict(color=f'{base_color} {0.2 + (i / num_runs) * 0.5})')  # varying opacity\n",
    "    ))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.arange(n_iter_comparison),\n",
    "    y=mean_single,\n",
    "    mode='lines',\n",
    "    name='Mean Loss',\n",
    "    line=dict(color='black', dash='dash')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate((np.arange(n_iter_comparison), np.arange(n_iter_comparison)[::-1])),\n",
    "    y=np.concatenate((mean_single + confidence_interval, (mean_single - confidence_interval)[::-1])),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(200, 200, 200, 0.3)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    name='Confidence Interval'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Optimization Runs with Mean and Confidence Intervals\",\n",
    "    xaxis=dict(\n",
    "        title=\"Iteration\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        gridcolor='rgba(211, 211, 211, 0.8)',  \n",
    "        linecolor='rgba(150, 150, 150, 0.8)',  \n",
    "        linewidth=1,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            size=12,\n",
    "            color='black'\n",
    "        ),\n",
    "        gridwidth=1,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Loss\",\n",
    "        showline=True,\n",
    "        showgrid=True,\n",
    "        gridcolor='rgba(211, 211, 211, 0.8)',  \n",
    "        linecolor='rgba(150, 150, 150, 0.8)',  \n",
    "        linewidth=1,\n",
    "        ticks='outside',\n",
    "        tickfont=dict(\n",
    "            size=12,\n",
    "            color='black'\n",
    "        ),\n",
    "        gridwidth=1,\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    width=1500,\n",
    "    height=650,\n",
    "    legend_title_text='Runs',\n",
    "    legend_font_size=12,\n",
    "    margin=dict(l=100, r=100, t=100, b=100),  \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## <strong> 4 | Benchmarking against Karnachos </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 4.0 | Initial Truss layout </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- KANARACHOS TRUSS ---------------------------- #\n",
    "%matplotlib widget\n",
    "read_file_path = write_file_path =  '../TRUSS1/truss_bridge/data'\n",
    "\n",
    "path_pro = os.path.normpath(os.path.join(read_file_path, 'bridge_frequency.pro'))\n",
    "props = pu.parse_file(path_pro)\n",
    "\n",
    "path_geom = os.path.normpath(os.path.join(read_file_path, 'bridge.geom'))\n",
    "props['model']['truss']['area'] = props['model']['truss']['area'] = [4e-3,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4,1e-4]\n",
    "props['init']['mesh']['file'] = path_geom\n",
    "\n",
    "trap = io.StringIO()\n",
    "with redirect_stdout(trap):\n",
    "    globdat = main.jive(props)\n",
    "\n",
    "# -------------------------- Initiate Initial truss -------------------------- #\n",
    "read_file = 'bridge.geom'\n",
    "write_file_path =  None\n",
    "write_file = None\n",
    "violation_factor = None\n",
    "freq_constraints = None\n",
    "mass_weight = None\n",
    "freq_weight = None\n",
    "scalers = [None,None]\n",
    "bounds = None\n",
    "\n",
    "TRUSS_initial = TRUSS(read_file_path,\n",
    "               read_file,\n",
    "                write_file_path,\n",
    "                write_file,\n",
    "                violation_factor,\n",
    "                scalers,\n",
    "                bounds,\n",
    "                freq_constraints= freq_constraints, \n",
    "                mass_weight= mass_weight, \n",
    "                freq_weight= freq_weight)\n",
    "TRUSS_initial.read_geom_file()\n",
    "\n",
    "y_coord = TRUSS_initial.state[\"y_coordinates\"][::2][1:6]\n",
    "areas_initial = np.array(props['model']['truss']['area'])\n",
    "print(f'MASS: {TRUSS_initial.mass_truss(areas_initial, y_coord)} kg  Natural Frequencies:{(globdat[gn.EIGENFREQS][0:3] / 2 / np.pi)} Hz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 4.1 | Final Trusses & Basic statistics </strong>\n",
    "\n",
    "The following are the displays of both solutions from Kanarachos et al., 2017 and TRUSS we can see that the overall shape is similar and as expected forms an incresingly round shape. By contrast the locations of members mass differ between trusses. Why this is the case is not entirely clear but for now it is understood that this is dominated by the converged solution iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ----------------------------- KANARACHOS TRUSS ---------------------------- #\n",
    "%matplotlib widget\n",
    "read_file_path = write_file_path =  '../TRUSS1/truss_bridge/data'\n",
    "\n",
    "path_pro = os.path.normpath(os.path.join(read_file_path, 'bridge_frequency_kr.pro'))\n",
    "props = pu.parse_file(path_pro)\n",
    "\n",
    "path_geom = os.path.normpath(os.path.join(read_file_path, 'bridge_kanarachos.geom'))\n",
    "props['model']['truss']['area'] = [ 40e-4, 3.1997e-4, 1.0025e-4, 1.0000e-4, 2.5875e-4, 1.0895e-4, 1.1261e-4, 2.5624e-4, 1.4121e-4, 1.5758e-4, 2.2461e-4, 1.0694e-4, 1.3193e-4, 2.3846e-4, 1.0001e-4 ]\n",
    "props['init']['mesh']['file'] = path_geom\n",
    "\n",
    "trap = io.StringIO()\n",
    "with redirect_stdout(trap):\n",
    "    globdat = main.jive(props)\n",
    "# ------------------------ Initiate a Karnachos obect ------------------------ #\n",
    "read_file_path = '../TRUSS1/truss_bridge/data'\n",
    "read_file = 'bridge_kanarachos.geom'\n",
    "write_file_path =  None\n",
    "write_file = None\n",
    "violation_factor = None\n",
    "freq_constraints = None\n",
    "mass_weight = None\n",
    "freq_weight = None\n",
    "scalers = [None,None]\n",
    "bounds = None\n",
    "\n",
    "TRUSSKN = TRUSS(read_file_path,\n",
    "               read_file,\n",
    "                write_file_path,\n",
    "                write_file,\n",
    "                violation_factor,\n",
    "                scalers,\n",
    "                bounds,\n",
    "                freq_constraints= freq_constraints, \n",
    "                mass_weight= mass_weight, \n",
    "                freq_weight= freq_weight)\n",
    "TRUSSKN.read_geom_file()\n",
    "\n",
    "y_coord = TRUSSKN.state[\"y_coordinates\"][::2][1:6]\n",
    "areas_kn = np.array(props['model']['truss']['area'])\n",
    "print(f'MASS: {TRUSSKN.mass_truss(areas_kn, y_coord)} kg  Natural Frequencies:{(globdat[gn.EIGENFREQS][0:3] / 2 / np.pi)} Hz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ---------------------------------- TRUSS1 ---------------------------------- #\n",
    "TRUSSDummy.config[\"write_file_path\"] = '../TRUSS1/truss_bridge/data/@TRUSS'\n",
    "TRUSSDummy.Write_call_state(X_init_single)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 4.2 | Comparing benchmarks </strong>\n",
    "\n",
    "From the following benchmarks we can show how the converged solutions are quite similar the TRUSS solution performs better in meeting the NF constraints but not as well in optimising for the mass which is the preferred optimisation variable.This is in part because the TRUSS solution loss function has been hyperparametrised giving a higher weight to the natural frequencies. This is in the experience from multiple runs and hyperparameter optimisation led to a much gaster convergence but may also restrict the final ability of the model to find an even better solution. Additionally, we have been able to see in the loss function figure how the setup of the loss function could be improved since the loss function minima does not correspond with the best found solution. Therefore further refining in the loss function may even yield better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mask = TRUSSDummy.state[\"monitor_df\"][\"Natural Frequencies\"][:].apply(lambda x: (x >= TRUSS1.config[\"freq_constraints\"]).all())\n",
    "ind = TRUSSDummy.state[\"monitor_df\"][mask][\"Mass of Truss\"].idxmin()\n",
    "# ----------------------------------- Data ----------------------------------- #\n",
    "TR_cs = TRUSSDummy.state[\"cross_sections\"].numpy() #To Numpy on first run (Tensor format)\n",
    "KN_cs = TRUSSKN.state[\"cross_sections\"]\n",
    "\n",
    "TR_m = TRUSSDummy.mass_truss(TR_cs, TRUSSDummy.state[\"y_coordinates\"][::2][1:6])\n",
    "KN_m = TRUSSKN.mass_truss(KN_cs, TRUSSKN.state[\"y_coordinates\"][::2][1:6])\n",
    "\n",
    "NFK = globdat[gn.EIGENFREQS][0:3]/2/np.pi\n",
    "NFTR = TRUSSDummy.state[\"monitor_df\"][\"Natural Frequencies\"][ind]\n",
    "\n",
    "data = {\n",
    "    'Model': ['Kanarachos model', 'TRUSS model'],\n",
    "    'Cross Section Average [m^2]': [KN_cs.mean(), TR_cs.mean()],\n",
    "    'Natural Frequency Average [rad/s]': [np.mean(NFTR), np.mean(NFK)],\n",
    "    'Mass [kg]': [KN_m, TR_m],\n",
    "    'Convergence time [s]': ['473', '315'],\n",
    "}\n",
    "basic_comp = pd.DataFrame(data)\n",
    "basic_comp.set_index('Model', inplace=True)\n",
    "basic_comp.index.name = None\n",
    "basic_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "max_cross_sections = max(np.max(TR_cs), np.max(KN_cs))\n",
    "normalized_cross_sections_model1 = TR_cs / max_cross_sections\n",
    "normalized_cross_sections_model2 = KN_cs / max_cross_sections\n",
    "\n",
    "max_mass = max(TR_m, KN_m)\n",
    "normalized_mass_model1 = TR_m / max_mass\n",
    "normalized_mass_model2 = KN_m / max_mass\n",
    "\n",
    "max_nat_freq = max(np.max(NFTR), np.max(NFK))\n",
    "normalized_nat_freq_model1 = np.array(NFTR) / max_nat_freq\n",
    "normalized_nat_freq_model2 = np.array(NFK) / max_nat_freq\n",
    "# ------------------------------------ VIS ----------------------------------- #\n",
    "# Data for plotting\n",
    "data = {\n",
    "    \"Metric\": [\"Cross Sections (Mean)\", \"Mass\", \"Nat. Freq. 1\", \"Nat. Freq. 2\", \"Nat. Freq. 3\"],\n",
    "    \"Model 1 (TRUSS1)\": [np.mean(normalized_cross_sections_model1), normalized_mass_model1] + normalized_nat_freq_model1.tolist(),\n",
    "    \"Model 2 (KANARACHOS)\": [np.mean(normalized_cross_sections_model2), normalized_mass_model2] + normalized_nat_freq_model2.tolist()\n",
    "}\n",
    "dfbenchmark = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "colors = ['#00B8C8', '#EC6842']  \n",
    "\n",
    "for i, model in enumerate(dfbenchmark.columns[1:]):\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=dfbenchmark['Metric'],\n",
    "        y=dfbenchmark[model],\n",
    "        name=model,\n",
    "        marker=dict(color=colors[i])\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Comparison of Normalized Cross Sections, Mass, and Natural Frequencies\",\n",
    "    xaxis=dict(\n",
    "        title=\"Metric\",\n",
    "        showgrid=True,  \n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Normalized Value\",\n",
    "        showgrid=True,  \n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    barmode='group',\n",
    "    legend_title_text='Model',\n",
    "    legend_font_size=12,\n",
    "    margin=dict(l=100, r=100, t=100, b=100),\n",
    "    template = \"simple_white\"  \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsaie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
